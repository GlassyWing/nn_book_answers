# 前馈神经网路

## 对于一个神经元σ(wTx + b)，并使用梯度下降优化参数w时，如果输入x恒大于0，其收敛速度会比零均值化的输入更慢。

当激活函数的输出恒大于0时，会造成后一层的神经元的输入发生偏置偏移

## 试设计一个前馈神经网络来解决XOR问题，要求该前馈神经网络具有两个隐藏神经元和一个输出神经元，并使用ReLU作为激活函数。

## 如果限制一个神经网络的总神经数量为N，层数为L，每个隐藏层的神经元数量为 N−1 / L−1 ，试分析参数数量和层数L的关系。

层l的参数数量为 $ num(w)= N_l (N_{l - 1}) $, 又 $ N_l = \frac{N-1}{l - 1} $, 则 $ num(w) = \frac{(N-1)^2}{(l-1)(l-2)} $

## 为什么在神经网络模型的结构化风险函数中不对偏置 b 进行正则化？

值得注意的是，有一个较大的偏置并不会使得神经元对它的输入像有大权重那样敏感。所以我们不用担心较大的偏置会使我们的网络学习训练数据中的噪声。同时，允许大规模的偏置使我们的网络在性能上更为灵活——特别是较大的偏置使得神经元更容易饱和，这通常是我们期望的。由于这些原因，我们通常不对偏置做正则化。

## 为什么在用反向传播算法进行参数学习时要采用随机参数初始化的方式而不是直接令W = 0, b = 0？

根据误差项计算公式，在参数为0的情况下，每一层的误差项都为0，致使反向传播不起作用，W, b永远无法更新。
