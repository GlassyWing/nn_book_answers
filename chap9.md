# 无监督学习

1. 分析主成分分析为什么具有数据降噪能力？
   噪音被认为是两个特征在该维度上的方差较小，而PCA后，选择的是方差较大的基向量，因此达到了去噪的效果。
2. 证明对于N 个样本（样本维数d > N）组成的数据集，主成分分析的有效投影子空间不超过N − 1 维。
   设矩阵A的维度为N x d, 那么A的秩R(A) <= N，那么实对称矩阵$ A^TA $的秩同样有$ R(A^TA) <= N $
3. 对于一个两类分类问题，试举例分析什么样的数据分布会使得主成分分析得到的特征反而会使得分类性能下降。
   流形数据分布（如sin曲线），这类分布无论在哪个维度上的投影都有重叠或者方差为0，无法进行分类。
4. 若数据矩阵X′ = X − ¯X ，则对X′ 奇异值分解X′ = UΣV ，则U为主成分分析的投影矩阵。
   $$ X^{'}=X-\overline{X} $$
   那么 
   $$ X^{'}X^{'^T} = U\Sigma VV^T\Sigma^T U^T = U\Sigma \Sigma^T U^T $$
   即为X的协方差，其中 $ \Sigma \Sigma^T $ 即为特征值对角矩阵，那么U即为所对应的主成分分析的投影矩阵。
5. 举例说明，K近邻方法估计的密度函数不是严格的概率密度函数，其在整个空间上的积分不等于1。
   以2维均匀分布为例，样本数量为N，此时圆的半径R和K的值可以固定，对于K近邻方法得到的点密度为
   $$ \frac{K}{N\pi R^2} $$
   那么整个空间上的积分为
   $$ \sum_{i=1}^N \frac{K}{N\pi R^2} =  \frac{K}{\pi R^2} != 1 $$
6. 对于一个C 类的分类问题，使用K近邻方法估计每个类c (1 ≤c ≤ C) 的密度函数p(x|c)，并使用贝叶斯公式计算每个类的后验概率p(c|x)。
   根据贝叶斯公式有：
   $$ p(c_i|x) = \frac{p(c_i,x)}{p(x)} \\ = \frac{p(x|c_i)p(c_i)}{p(x)} \\ = \frac{p(x|c_i)p(c_i)}{\sum_{i=1}^np(x|c_i)p(c_i)} $$
   那么只需要估算出$ p(c_i) $