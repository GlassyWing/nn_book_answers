# 第三章-线性模型

## 证明在两类线性分类中，权重向量w与决策平面正交。

决策平面：$ w^Tx - b = 0 $

b 只是表示从原点到平面的距离，将平面移至原点： $ w^Tx = 0 $

根据两向量正交的原理，得知权重向量 $ w $ 与 决策平面正交

## 在线性空间中，证明一个点x到平面 $ f(x, w) = w^Tx + b = 0 $的距离为$ \frac{|f(x,w)|}{∥w∥} $。

根据点到平面的距离公式

$ S = \frac{|Ax+By+Cz+D|}{\sqrt{A^2+B^2+C^2}} $

此外，决策平面的法向量即为 $ W $, 可知

$ S = \frac{f(x,w)}{∥w∥} $

## 在多类分类中，1）如果一个数据集中每个类的样本都可与其它类是线性可分的，则该数据集一定是线性可分的；2）如果一个数据集中每两个类的样本是线性可分的，则该数据集不一定是线性可分的。

1）将整个数据集按照二分类划分，并对其他类递归进行划分，必然可以得到所对应的类型，所以一定是线性可分的
2）问题等效于在 （A, B）与 （C，D）类数据集线性可分的条件下，A与B类数据集是否可分，显然，A与B不一定线性可分

## 在logistic 回归中，是否可以用 $ \hat{y} = \sigma(w^Tx) $去逼近正确的标签y，并用平方损失 $ (\hat{y} - y) ^2 $ 最小化来优化参数w？

不行，因为存在梯度消失问题
$$ loss = \sum_{i}^N{(y_i - \sigma(w^Tx_i))^2} $$

$$ \frac{\partial{loss}}{\partial{w}} = \sum_i^N{-2(y_i - \sigma(w^Tx_i))\sigma(w^Tx_i)(1-\sigma(w^Tx_i))x_i} $$

由于 $ \sigma(w^Tx_i) $的优化目标是接近 $ y_i$, 因此 $ \sigma(w^Tx_i) $ 和 $ (1 - \sigma(w^Tx_i)) $ 中的一个会越来越接近0，即梯度消失。

## 在softmax 回归的风险函数（公式(3.39)）中，如果去掉正则化项会有什么影响？

造成过拟合