# 卷积神经网路

## 证明宽卷积具有交换性，即公式(5.10)。

根据卷积定义有：

$$ W\bigotimes X = \sum_{u}\sum_{v}w_{uv} \cdot x_{i+u-1,j+v-1} $$

而

$$ X \bigotimes W = \sum_{u}\sum_{v} x_{i+u-1,j+v-1} \cdot w_{uv} $$

显然 $ W\bigotimes X = X \bigotimes W$

## 分析卷积神经网络中用1 × 1的滤波器的作用。

1. 替换全连接层
2. 降维/升维（减少参数数量）

## 对于一个输入为100 × 100 × 256的特征映射组，使用3 × 3的卷积核，输出为100 × 100 × 256的特征映射组的卷积层，求其时间和空间复杂度。如果引入一个1 × 1卷积核先得到100 × 100 × 64的特征映射，再进行3 × 3的卷积，得到100 × 100 × 256的特征映射组，求其时间和空间复杂度。

1. 由于是same变换，时间复杂度为100 x 100 x 3 x 3 x 256 x 256, 空间复杂度即参数数量为 3 x 3 x 256 x 256
2. 时间复杂度为 (100 x 100 x 1 x 1 x 256 x 64 + 100 x 100 x 3 x 3 x 64 x 256) = 100 x 100 x 10 x 256 x 64，空间复杂度为 (1 x 1 x 256 x 64 + 3 x 3 x 64 x 256) = 10 x 256 x 64，差了3.6倍。

## 对于一个两维卷积，输入为3 × 3，卷积核大小为2 × 2，试将卷积操作重写为仿射变换的形式。

$$ C^T = \begin{bmatrix}
    w_{0,0} & 0 & 0 & 0 \\
    w_{0,1} & w_{0,0} & 0 & 0 \\
    0 & w_{0,1} & 0 & 0 \\
    w_{1,0} & 0 & w_{0,0} & 0 \\
    w_{1,1} & w_{1,0} & w_{0,1} & w_{0,0} \\
    0 & w_{1, 1} & 0 & w_{0, 1} \\
    0 & 0 & w_{1,0} & 0 \\
    0 & 0 & w_{1,1} & w_{1,0} \\
    0 & 0 & 0 & w_{1, 1}
\end{bmatrix} $$

$$ y = w \bigotimes x = Cx $$

其中：x为输入（9x1)，y为输出(4x1)， C为4x9的矩阵。

此线性操作把一个9维的输入向量，经过卷积运算后输出一个4维的向量，最后再转变为一个2x2的输出矩阵。

## 忽略激活函数，分析卷积网络中卷积层的前向计算和反向传播（公式(5.36)）是一种转置关系。

忽略激活函数等效于激活函数为恒定函数，它的导数恒为1，根据公式5.36可知反向传播过程与前向计算过程是转置关系，这里的转置是一种形式上的关系，并不是真正的转置。

## 在空洞卷积中，当卷积核大小为m，膨胀率为d时，如何设置零填充p的值以使得卷积为等宽卷积。

由于

$$ m' = m + (m-1)(d-1) $$

而要使其为等宽卷积，需满足

$$ \frac{n + 2p - m'}{s} + 1 = n $$ 假设s=1，即：

$$ n + 2p - m' + 1 = n$$ 求得

$$ p = \frac{1+m'}{2} = \frac{1+m+(m-1)(d-1)}{2} = 1 + \frac{(m-1)d}{2} $$
